import tensorflow as TF
import lists as L
include string-dict

type Tensor = TF.Tensor
type Model = TF.Model
type Sequential = TF.Sequential

DATASET = "Though my head is often in security, networking, formal methods, and HCI, my heart is in programming languages. Over the years I have contributed to several innovative and useful software systems: JavaScript and Web tools, Flowlog and related tools, Racket (formerly DrScheme), WeScheme, Margrave, Flapjax, FrTime, Continue, FASTLINK, (Per)Mission, and more. For some of what I've been doing lately, please see my research group's blog. Recently, I have decided to devote a substantial portion of my time and energy to the hardest problem I've worked on: computer science education. It's the hardest because it requires substantial work on both technical and human-factors fronts; the audience is often unsophisticated and vulnerable; and if you screw up, you can do real damage to not only individuals but also the field and society. I recently wrote up a manifesto for my new direction [the same text is on both Facebook and Google+]. Since 1995, decades before it became a fashionable hobby, I've been devoted to computer science outreach at a national (and larger) scale. My collaborators and I wrote a best-selling book and created a series of successful outreach programs. I also wrote the widely-used Programming Languages: Application and Interpretation. The current outreach program, Bootstrap, is used internationally for both computing and math education, and has been adopted as the middle-school mathematics curriculum by Code.org. I'm expanding Bootstrap's reach to also interface with physics and data science. As part of these projects, I am working on the Pyret programming language and a new book, Programming and Programming Languages. I also teach in Brown's Executive Master in Cybersecurity program, where I'm responsible for the human factors course. I'm honored to be a recipient of SIGPLAN's Robin Milner Young Researcher Award, SIGSOFT's Influential Educator Award, and Brown University's Wriston Fellowship. Disclosure: My work has been supported financially by the US National Science Foundation, Bloomberg, Cisco, Code.org, CSNYC, the ESA Foundation, Fujitsu, General Motors, Google, Infosys, Jane Street Capital, the State of Rhode Island, and TripAdvisor. I believe my views have not been swayed by this support, but I provide this information so you can judge for yourself. Bootstrap:Algebra is a curricular module designed to integrate introductory computing into an algebra class; the module aims to help students improve on various essential learning outcomes from state and national algebra standards. In prior work, we published initial findings about student performance gains on algebra problems after taking Bootstrap. While the results were promising, the dataset was not large, and had students working on algebra problems that had been scaffolded with Bootstrap's pedagogy. This paper reports on a more detailed study with (a) data from more than three times as many students, (b) analysis of performance changes in incorrect answers, (c) some problems in which the Bootstrap scaffolds have been removed, and (d) an IRT analysis across the elements of Bootstrap's program-design pedagogy. Our results confirm that students improve on algebraic word problems after completing the module, even on unscaffolded problems. The nature of incorrect answers to symbolic-form questions also appears to improve after Bootstrap. Game programming projects are concrete and motivational for students, especially when used to teach more abstract concepts such as algebra. These projects must have open-ended elements to allow for creativity, but too much freedom makes it hard to reach specific learning outcomes. How many degrees of freedom do students need to make a game feel like one they genuinely designed? What kinds of personalization do they undertake of their games? And how do these factors correlate with their prior game-playing experience or with their identified gender? This paper studies these questions in the concrete setting of the Bootstrap:Algebra curriculum. In this curriculum, students are only given four parameters they can customize and only a few minutes in which to do so. Our study shows that despite this very limited personalization, students still feel a strong sense of ownership, originality, and pride in their creations. We also find that females find videogame creation just as satisfying as males, which contradicts some prior research but may also reflect the nature of games created in this curriculum and the opportunities it offers for self-expression. We evaluate a notional machine for recursion based on algebraic substitution. To do this, we decompose recursion into a progression of function call patterns, parameter name reuse, and data structure complexity. At each stage, we test students' ability to trace programs using substitution. We evaluate the correctness of their traces along multiple dimensions, finding that students generally do well, and also observe shortcuts and identify misconceptions. For comparison, we also have students trace two problems using a traditional, imperative notional machine. Even though the substitution model is unwieldy to use with compound data, students still perform better with it than with the traditional notional machine. Model-finding tools like the Alloy Analyzer produce concrete examples of how a declarative specification can be satisfied. These formal tools are useful in a wide range of domains: software design, security, networking, and more. By producing concrete examples, they assist in exploring system behavior and can help find surprising faults. Specifications usually have many potential candidate solutions, but model-finders tend to leave the choice of which examples to present entirely to the underlying solver. This paper closes that gap by exploring notions of coverage for the model-finding domain, yielding a novel, rigorous metric for output quality. These ideas are realized in the tool CompoSAT, which interposes itself between Alloy's constraint-solving and presentation stages to produce ensembles of examples that maximize coverage. We show that high-coverage ensembles like CompoSAT produces are useful for, among other things, detecting overconstraint---a particularly insidious form of specification error. We detail the underlying theory and implementation of CompoSAT and evaluate it on numerous specifications. Instructors routinely use automated assessment methods to evaluate the semantic qualities of student implementations and, sometimes, test suites. In this work, we distill a variety of automated assessment methods in the literature down to a pair of assessment models. We identify pathological assessment outcomes in each model that point to underlying methodological flaws. These theoretical flaws broadly threaten the validity of the techniques, and we actually observe them in multiple assignments of an introductory programming course. We propose adjustments that remedy these flaws and then demonstrate, on these same assignments, that our interventions improve the accuracy of assessment. We believe that with these adjustments, instructors can greatly improve the accuracy of automated assessment. Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers inherit the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to ten programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems. Type systems and syntactic sugar are both valuable to programmers, but sometimes at odds. While sugar is a valuable mechanism for implementing realistic languages, the expansion process obscures program source structure. As a result, type errors can reference terms the programmers did not write (and even constructs they do not know), baffling them. The language developer must also manually construct type rules for the sugars, to give a typed account of the surface language. We address these problems by presenting a process for automatically reconstructing type rules for the surface language using rules for the core. We have implemented this theory, and show several interesting case studies. Scope, aliasing, mutation, and parameter passing are fundamental programming concepts that interact in subtle ways, especially in complex programs. Research has shown that students have substantial misconceptions on these topics. But this research has been done largely in CS1 courses, when students' programming experience is limited and problems are necessarily simple. What happens later in the curriculum? Does more programming experience iron out these misconceptions naturally, or are interventions required? This paper explores students' understanding of these topics in the context of a programming languages class for third- and fourth-year CS majors. Our pre- and post-tests pose questions in two programming languages to gauge whether upper-level students transfer knowledge between languages. Many students held misconceptions about these concepts at the start of the course. Students made progress in only some languages and topics, and cross-language transfer does not occur naturally. We also discuss various pedagogic activities we used to engage students with these concepts, and provide data and student opinion on their effectiveness. Scenario-finding tools like the Alloy Analyzer are widely used in numerous concrete domains like security, network analysis, UML analysis, and so on. They can help to verify properties and, more generally, aid in exploring a system's behavior. While scenario finders are valuable for their ability to produce concrete examples, individual scenarios only give insight into what is possible, leaving the user to make their own conclusions about what might be necessary. This paper enriches scenario finding by allowing users to ask ''why?'' and ''why not?'' questions about the examples they are given. We show how to distinguish parts of an example that cannot be consistently removed (or changed) from those that merely reflect underconstraint in the specification. In the former case we show how to determine which elements of the specification and which other components of the example together explain the presence of such facts. This paper formalizes the act of computing provenance in scenario-finding. We present Amalgam, an extension of the popular Alloy scenario-finder, which implements these foundations and provides interactive exploration of examples. We also evaluate Amalgam's algorithmics on a variety of both textbook and real-world examples. Most programming problems have multiple viable solutions that organize the underlying problem's tasks in fundamentally different ways. Which organizations (a.k.a. plans) students implement and prefer depends on solutions they have seen before as well as features of their programming language. How much exposure to planning do students need before they can appreciate and produce different plans? We report on a study in which students in introductory courses at two universities were given a single lecture on planning between assessments. In the post-assessment, many students produced multiple high-level plans (including ones first introduced in the lecture) and richly discussed tradeoffs between plans. This suggests that planning can be taught with fairly low overhead once students have a decent foundation in programming. Most programming languages have been designed by committees or individuals. What happens if, instead, we throw open the design process and let lots of programmers weigh in on semantic choices? Will they avoid well-known mistakes like dynamic scope? What do they expect of aliasing? What kind of overloading behavior will they choose? We investigate this issue by posing questions to programmers on Amazon Mechanical Turk. We examine several language features, in each case using multiple-choice questions to explore programmer preferences. We check the responses for consensus (agreement between people) and consistency (agreement across responses from one person). In general we find low consistency and consensus, potential confusion over mainstream features, and arguably poor design choices. In short, this preliminary evidence does not argue in favor of designing languages based on programmer preference. Sometime during the semester you will want to ask us questions about code. You might even think you've found a problem with a system being used in this course, and be inclined to submit a bug report. If you think you've found a problem, please do submit a report: you're doing us a favor, and we're grateful for it. However, before you do, please submit a good bug report. We'll write the rest of this document in terms of bug reporting, but the same principles apply even to asking a question. What follows below is general advice, not just for this course but for any system you might want to write a report about, ever. It's written in terms that make sense for programming tools, but you can generalize them to other systems without much trouble. A developer usually initially looks at the first two parts. More times than you might think, the seen output is actually correct. Then the developer knows that the problem is to educate you on why the system is behaving correctly, and your statement of why you thought it should do something else helps them understand your misunderstanding. If, however, the developer agrees that the seen output is not correct, now they have a problem to diagnose. Again, they will use all three parts. The first lets the developer try to get into the same state you're in. The second lets the developer confirm that they're in the same state. The third lets them understand what you think is wrong with this state. Yes, doing this last step is onerous. It's tempting to make it someone else's problem. But the more you make it someone else's problem, the longer it will take them to respond. Indeed, you should assume an exponential law: for every extra line of code, it may take twice as long to get a response. That's because your program, which is completely obvious to you, may take quite a while for your recipient to take into their heads and process. They don't know what's essential and what's incidental. So, before you submit the report, pare down. In fact, ideally, start from the empty buffer and add only what is absolutely necessary to manifest the error. As a bonus (to both the recipient and you), as you do this, sometimes you'll find that the actual problem was a bug in your code, not in the system that you're trying to report. For Pyret, you can report bugs directly, though you may want to check whether a similar issue has already been filed. Beware that any code you submit will be publicly visible. This course's primary learning goal is to teach you foundational programming skills in the context of algorithms and data structures. The course does this by interleaving the two and often using each one to motivate the other. Except where otherwise indicated, the sole assessment in the course will be homework assignments. These are take-home and open book relative to the policy on Honesty and Sharing. Occasionally you will be asked to do some readings, especially if they are found relevant based on questions and discussion in class. In keeping with the learning goals, the assignments focus on programming, algorithmics, or both. Usually you will implement a solution to some task, and then analyze its complexity. The purpose of these is to help you gain facility with thinking of software systems from the perspectives of both implementation and analysis. The work load in the course is uniformly distributed across the semester. Students can expect to spend about 10 hours each week on assignments. Combined with the 2.5 hours spent per week in class, this translates to approximately 180 hours over the course of the semester. Though my head is often in security, networking, formal methods, and HCI, my heart is in programming languages. Over the years I have contributed to several innovative and useful software systems: JavaScript and Web tools, Flowlog and related tools, Racket (formerly DrScheme), WeScheme, Margrave, Flapjax, FrTime, Continue, FASTLINK, (Per)Mission, and more. For some of what I've been doing lately, please see my research group's blog. Recently, I have decided to devote a substantial portion of my time and energy to the hardest problem I've worked on: computer science education. It's the hardest because it requires substantial work on both technical and human-factors fronts; the audience is often unsophisticated and vulnerable; and if you screw up, you can do real damage to not only individuals but also the field and society. I recently wrote up a manifesto for my new direction [the same text is on both Facebook and Google+]. Since 1995, decades before it became a fashionable hobby, I've been devoted to computer science outreach at a national (and larger) scale. My collaborators and I wrote a best-selling book and created a series of successful outreach programs. I also wrote the widely-used Programming Languages: Application and Interpretation. The current outreach program, Bootstrap, is used internationally for both computing and math education, and has been adopted as the middle-school mathematics curriculum by Code.org. I'm expanding Bootstrap's reach to also interface with physics and data science. As part of these projects, I am working on the Pyret programming language and a new book, Programming and Programming Languages. I also teach in Brown's Executive Master in Cybersecurity program, where I'm responsible for the human factors course. I'm honored to be a recipient of SIGPLAN's Robin Milner Young Researcher Award, SIGSOFT's Influential Educator Award, and Brown University's Wriston Fellowship. Disclosure: My work has been supported financially by the US National Science Foundation, Bloomberg, Cisco, Code.org, CSNYC, the ESA Foundation, Fujitsu, General Motors, Google, Infosys, Jane Street Capital, the State of Rhode Island, and TripAdvisor. I believe my views have not been swayed by this support, but I provide this information so you can judge for yourself. Bootstrap:Algebra is a curricular module designed to integrate introductory computing into an algebra class; the module aims to help students improve on various essential learning outcomes from state and national algebra standards. In prior work, we published initial findings about student performance gains on algebra problems after taking Bootstrap. While the results were promising, the dataset was not large, and had students working on algebra problems that had been scaffolded with Bootstrap's pedagogy. This paper reports on a more detailed study with (a) data from more than three times as many students, (b) analysis of performance changes in incorrect answers, (c) some problems in which the Bootstrap scaffolds have been removed, and (d) an IRT analysis across the elements of Bootstrap's program-design pedagogy. Our results confirm that students improve on algebraic word problems after completing the module, even on unscaffolded problems. The nature of incorrect answers to symbolic-form questions also appears to improve after Bootstrap. Game programming projects are concrete and motivational for students, especially when used to teach more abstract concepts such as algebra. These projects must have open-ended elements to allow for creativity, but too much freedom makes it hard to reach specific learning outcomes. How many degrees of freedom do students need to make a game feel like one they genuinely designed? What kinds of personalization do they undertake of their games? And how do these factors correlate with their prior game-playing experience or with their identified gender? This paper studies these questions in the concrete setting of the Bootstrap:Algebra curriculum. In this curriculum, students are only given four parameters they can customize and only a few minutes in which to do so. Our study shows that despite this very limited personalization, students still feel a strong sense of ownership, originality, and pride in their creations. We also find that females find videogame creation just as satisfying as males, which contradicts some prior research but may also reflect the nature of games created in this curriculum and the opportunities it offers for self-expression. We evaluate a notional machine for recursion based on algebraic substitution. To do this, we decompose recursion into a progression of function call patterns, parameter name reuse, and data structure complexity. At each stage, we test students' ability to trace programs using substitution. We evaluate the correctness of their traces along multiple dimensions, finding that students generally do well, and also observe shortcuts and identify misconceptions. For comparison, we also have students trace two problems using a traditional, imperative notional machine. Even though the substitution model is unwieldy to use with compound data, students still perform better with it than with the traditional notional machine. Model-finding tools like the Alloy Analyzer produce concrete examples of how a declarative specification can be satisfied. These formal tools are useful in a wide range of domains: software design, security, networking, and more. By producing concrete examples, they assist in exploring system behavior and can help find surprising faults. Specifications usually have many potential candidate solutions, but model-finders tend to leave the choice of which examples to present entirely to the underlying solver. This paper closes that gap by exploring notions of coverage for the model-finding domain, yielding a novel, rigorous metric for output quality. These ideas are realized in the tool CompoSAT, which interposes itself between Alloy's constraint-solving and presentation stages to produce ensembles of examples that maximize coverage. We show that high-coverage ensembles like CompoSAT produces are useful for, among other things, detecting overconstraint---a particularly insidious form of specification error. We detail the underlying theory and implementation of CompoSAT and evaluate it on numerous specifications. Instructors routinely use automated assessment methods to evaluate the semantic qualities of student implementations and, sometimes, test suites. In this work, we distill a variety of automated assessment methods in the literature down to a pair of assessment models. We identify pathological assessment outcomes in each model that point to underlying methodological flaws. These theoretical flaws broadly threaten the validity of the techniques, and we actually observe them in multiple assignments of an introductory programming course. We propose adjustments that remedy these flaws and then demonstrate, on these same assignments, that our interventions improve the accuracy of assessment. We believe that with these adjustments, instructors can greatly improve the accuracy of automated assessment. Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers inherit the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to ten programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems. Type systems and syntactic sugar are both valuable to programmers, but sometimes at odds. While sugar is a valuable mechanism for implementing realistic languages, the expansion process obscures program source structure. As a result, type errors can reference terms the programmers did not write (and even constructs they do not know), baffling them. The language developer must also manually construct type rules for the sugars, to give a typed account of the surface language. We address these problems by presenting a process for automatically reconstructing type rules for the surface language using rules for the core. We have implemented this theory, and show several interesting case studies. Scope, aliasing, mutation, and parameter passing are fundamental programming concepts that interact in subtle ways, especially in complex programs. Research has shown that students have substantial misconceptions on these topics. But this research has been done largely in CS1 courses, when students' programming experience is limited and problems are necessarily simple. What happens later in the curriculum? Does more programming experience iron out these misconceptions naturally, or are interventions required? This paper explores students' understanding of these topics in the context of a programming languages class for third- and fourth-year CS majors. Our pre- and post-tests pose questions in two programming languages to gauge whether upper-level students transfer knowledge between languages. Many students held misconceptions about these concepts at the start of the course. Students made progress in only some languages and topics, and cross-language transfer does not occur naturally. We also discuss various pedagogic activities we used to engage students with these concepts, and provide data and student opinion on their effectiveness. Scenario-finding tools like the Alloy Analyzer are widely used in numerous concrete domains like security, network analysis, UML analysis, and so on. They can help to verify properties and, more generally, aid in exploring a system's behavior. While scenario finders are valuable for their ability to produce concrete examples, individual scenarios only give insight into what is possible, leaving the user to make their own conclusions about what might be necessary. This paper enriches scenario finding by allowing users to ask ''why?'' and ''why not?'' questions about the examples they are given. We show how to distinguish parts of an example that cannot be consistently removed (or changed) from those that merely reflect underconstraint in the specification. In the former case we show how to determine which elements of the specification and which other components of the example together explain the presence of such facts. This paper formalizes the act of computing provenance in scenario-finding. We present Amalgam, an extension of the popular Alloy scenario-finder, which implements these foundations and provides interactive exploration of examples. We also evaluate Amalgam's algorithmics on a variety of both textbook and real-world examples. Most programming problems have multiple viable solutions that organize the underlying problem's tasks in fundamentally different ways. Which organizations (a.k.a. plans) students implement and prefer depends on solutions they have seen before as well as features of their programming language. How much exposure to planning do students need before they can appreciate and produce different plans? We report on a study in which students in introductory courses at two universities were given a single lecture on planning between assessments. In the post-assessment, many students produced multiple high-level plans (including ones first introduced in the lecture) and richly discussed tradeoffs between plans. This suggests that planning can be taught with fairly low overhead once students have a decent foundation in programming. Most programming languages have been designed by committees or individuals. What happens if, instead, we throw open the design process and let lots of programmers weigh in on semantic choices? Will they avoid well-known mistakes like dynamic scope? What do they expect of aliasing? What kind of overloading behavior will they choose? We investigate this issue by posing questions to programmers on Amazon Mechanical Turk. We examine several language features, in each case using multiple-choice questions to explore programmer preferences. We check the responses for consensus (agreement between people) and consistency (agreement across responses from one person). In general we find low consistency and consensus, potential confusion over mainstream features, and arguably poor design choices. In short, this preliminary evidence does not argue in favor of designing languages based on programmer preference. Sometime during the semester you will want to ask us questions about code. You might even think you've found a problem with a system being used in this course, and be inclined to submit a bug report. If you think you've found a problem, please do submit a report: you're doing us a favor, and we're grateful for it. However, before you do, please submit a good bug report. We'll write the rest of this document in terms of bug reporting, but the same principles apply even to asking a question. What follows below is general advice, not just for this course but for any system you might want to write a report about, ever. It's written in terms that make sense for programming tools, but you can generalize them to other systems without much trouble. A developer usually initially looks at the first two parts. More times than you might think, the seen output is actually correct. Then the developer knows that the problem is to educate you on why the system is behaving correctly, and your statement of why you thought it should do something else helps them understand your misunderstanding. If, however, the developer agrees that the seen output is not correct, now they have a problem to diagnose. Again, they will use all three parts. The first lets the developer try to get into the same state you're in. The second lets the developer confirm that they're in the same state. The third lets them understand what you think is wrong with this state. Yes, doing this last step is onerous. It's tempting to make it someone else's problem. But the more you make it someone else's problem, the longer it will take them to respond. Indeed, you should assume an exponential law: for every extra line of code, it may take twice as long to get a response. That's because your program, which is completely obvious to you, may take quite a while for your recipient to take into their heads and process. They don't know what's essential and what's incidental. So, before you submit the report, pare down. In fact, ideally, start from the empty buffer and add only what is absolutely necessary to manifest the error. As a bonus (to both the recipient and you), as you do this, sometimes you'll find that the actual problem was a bug in your code, not in the system that you're trying to report. For Pyret, you can report bugs directly, though you may want to check whether a similar issue has already been filed. Beware that any code you submit will be publicly visible. This course's primary learning goal is to teach you foundational programming skills in the context of algorithms and data structures. The course does this by interleaving the two and often using each one to motivate the other. Except where otherwise indicated, the sole assessment in the course will be homework assignments. These are take-home and open book relative to the policy on Honesty and Sharing. Occasionally you will be asked to do some readings, especially if they are found relevant based on questions and discussion in class. In keeping with the learning goals, the assignments focus on programming, algorithmics, or both. Usually you will implement a solution to some task, and then analyze its complexity. The purpose of these is to help you gain facility with thinking of software systems from the perspectives of both implementation and analysis. The work load in the course is uniformly distributed across the semester. Students can expect to spend about 10 hours each week on assignments. Combined with the 2.5 hours spent per week in class, this translates to approximately 180 hours over the course of the semester. Though my head is often in security, networking, formal methods, and HCI, my heart is in programming languages. Over the years I have contributed to several innovative and useful software systems: JavaScript and Web tools, Flowlog and related tools, Racket (formerly DrScheme), WeScheme, Margrave, Flapjax, FrTime, Continue, FASTLINK, (Per)Mission, and more. For some of what I've been doing lately, please see my research group's blog. Recently, I have decided to devote a substantial portion of my time and energy to the hardest problem I've worked on: computer science education. It's the hardest because it requires substantial work on both technical and human-factors fronts; the audience is often unsophisticated and vulnerable; and if you screw up, you can do real damage to not only individuals but also the field and society. I recently wrote up a manifesto for my new direction [the same text is on both Facebook and Google+]. Since 1995, decades before it became a fashionable hobby, I've been devoted to computer science outreach at a national (and larger) scale. My collaborators and I wrote a best-selling book and created a series of successful outreach programs. I also wrote the widely-used Programming Languages: Application and Interpretation. The current outreach program, Bootstrap, is used internationally for both computing and math education, and has been adopted as the middle-school mathematics curriculum by Code.org. I'm expanding Bootstrap's reach to also interface with physics and data science. As part of these projects, I am working on the Pyret programming language and a new book, Programming and Programming Languages. I also teach in Brown's Executive Master in Cybersecurity program, where I'm responsible for the human factors course. I'm honored to be a recipient of SIGPLAN's Robin Milner Young Researcher Award, SIGSOFT's Influential Educator Award, and Brown University's Wriston Fellowship. Disclosure: My work has been supported financially by the US National Science Foundation, Bloomberg, Cisco, Code.org, CSNYC, the ESA Foundation, Fujitsu, General Motors, Google, Infosys, Jane Street Capital, the State of Rhode Island, and TripAdvisor. I believe my views have not been swayed by this support, but I provide this information so you can judge for yourself. Bootstrap:Algebra is a curricular module designed to integrate introductory computing into an algebra class; the module aims to help students improve on various essential learning outcomes from state and national algebra standards. In prior work, we published initial findings about student performance gains on algebra problems after taking Bootstrap. While the results were promising, the dataset was not large, and had students working on algebra problems that had been scaffolded with Bootstrap's pedagogy. This paper reports on a more detailed study with (a) data from more than three times as many students, (b) analysis of performance changes in incorrect answers, (c) some problems in which the Bootstrap scaffolds have been removed, and (d) an IRT analysis across the elements of Bootstrap's program-design pedagogy. Our results confirm that students improve on algebraic word problems after completing the module, even on unscaffolded problems. The nature of incorrect answers to symbolic-form questions also appears to improve after Bootstrap. Game programming projects are concrete and motivational for students, especially when used to teach more abstract concepts such as algebra. These projects must have open-ended elements to allow for creativity, but too much freedom makes it hard to reach specific learning outcomes. How many degrees of freedom do students need to make a game feel like one they genuinely designed? What kinds of personalization do they undertake of their games? And how do these factors correlate with their prior game-playing experience or with their identified gender? This paper studies these questions in the concrete setting of the Bootstrap:Algebra curriculum. In this curriculum, students are only given four parameters they can customize and only a few minutes in which to do so. Our study shows that despite this very limited personalization, students still feel a strong sense of ownership, originality, and pride in their creations. We also find that females find videogame creation just as satisfying as males, which contradicts some prior research but may also reflect the nature of games created in this curriculum and the opportunities it offers for self-expression. We evaluate a notional machine for recursion based on algebraic substitution. To do this, we decompose recursion into a progression of function call patterns, parameter name reuse, and data structure complexity. At each stage, we test students' ability to trace programs using substitution. We evaluate the correctness of their traces along multiple dimensions, finding that students generally do well, and also observe shortcuts and identify misconceptions. For comparison, we also have students trace two problems using a traditional, imperative notional machine. Even though the substitution model is unwieldy to use with compound data, students still perform better with it than with the traditional notional machine. Model-finding tools like the Alloy Analyzer produce concrete examples of how a declarative specification can be satisfied. These formal tools are useful in a wide range of domains: software design, security, networking, and more. By producing concrete examples, they assist in exploring system behavior and can help find surprising faults. Specifications usually have many potential candidate solutions, but model-finders tend to leave the choice of which examples to present entirely to the underlying solver. This paper closes that gap by exploring notions of coverage for the model-finding domain, yielding a novel, rigorous metric for output quality. These ideas are realized in the tool CompoSAT, which interposes itself between Alloy's constraint-solving and presentation stages to produce ensembles of examples that maximize coverage. We show that high-coverage ensembles like CompoSAT produces are useful for, among other things, detecting overconstraint---a particularly insidious form of specification error. We detail the underlying theory and implementation of CompoSAT and evaluate it on numerous specifications. Instructors routinely use automated assessment methods to evaluate the semantic qualities of student implementations and, sometimes, test suites. In this work, we distill a variety of automated assessment methods in the literature down to a pair of assessment models. We identify pathological assessment outcomes in each model that point to underlying methodological flaws. These theoretical flaws broadly threaten the validity of the techniques, and we actually observe them in multiple assignments of an introductory programming course. We propose adjustments that remedy these flaws and then demonstrate, on these same assignments, that our interventions improve the accuracy of assessment. We believe that with these adjustments, instructors can greatly improve the accuracy of automated assessment. Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers inherit the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to ten programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems. Type systems and syntactic sugar are both valuable to programmers, but sometimes at odds. While sugar is a valuable mechanism for implementing realistic languages, the expansion process obscures program source structure. As a result, type errors can reference terms the programmers did not write (and even constructs they do not know), baffling them. The language developer must also manually construct type rules for the sugars, to give a typed account of the surface language. We address these problems by presenting a process for automatically reconstructing type rules for the surface language using rules for the core. We have implemented this theory, and show several interesting case studies. Scope, aliasing, mutation, and parameter passing are fundamental programming concepts that interact in subtle ways, especially in complex programs. Research has shown that students have substantial misconceptions on these topics. But this research has been done largely in CS1 courses, when students' programming experience is limited and problems are necessarily simple. What happens later in the curriculum? Does more programming experience iron out these misconceptions naturally, or are interventions required? This paper explores students' understanding of these topics in the context of a programming languages class for third- and fourth-year CS majors. Our pre- and post-tests pose questions in two programming languages to gauge whether upper-level students transfer knowledge between languages. Many students held misconceptions about these concepts at the start of the course. Students made progress in only some languages and topics, and cross-language transfer does not occur naturally. We also discuss various pedagogic activities we used to engage students with these concepts, and provide data and student opinion on their effectiveness. Scenario-finding tools like the Alloy Analyzer are widely used in numerous concrete domains like security, network analysis, UML analysis, and so on. They can help to verify properties and, more generally, aid in exploring a system's behavior. While scenario finders are valuable for their ability to produce concrete examples, individual scenarios only give insight into what is possible, leaving the user to make their own conclusions about what might be necessary. This paper enriches scenario finding by allowing users to ask ''why?'' and ''why not?'' questions about the examples they are given. We show how to distinguish parts of an example that cannot be consistently removed (or changed) from those that merely reflect underconstraint in the specification. In the former case we show how to determine which elements of the specification and which other components of the example together explain the presence of such facts. This paper formalizes the act of computing provenance in scenario-finding. We present Amalgam, an extension of the popular Alloy scenario-finder, which implements these foundations and provides interactive exploration of examples. We also evaluate Amalgam's algorithmics on a variety of both textbook and real-world examples. Most programming problems have multiple viable solutions that organize the underlying problem's tasks in fundamentally different ways. Which organizations (a.k.a. plans) students implement and prefer depends on solutions they have seen before as well as features of their programming language. How much exposure to planning do students need before they can appreciate and produce different plans? We report on a study in which students in introductory courses at two universities were given a single lecture on planning between assessments. In the post-assessment, many students produced multiple high-level plans (including ones first introduced in the lecture) and richly discussed tradeoffs between plans. This suggests that planning can be taught with fairly low overhead once students have a decent foundation in programming. Most programming languages have been designed by committees or individuals. What happens if, instead, we throw open the design process and let lots of programmers weigh in on semantic choices? Will they avoid well-known mistakes like dynamic scope? What do they expect of aliasing? What kind of overloading behavior will they choose? We investigate this issue by posing questions to programmers on Amazon Mechanical Turk. We examine several language features, in each case using multiple-choice questions to explore programmer preferences. We check the responses for consensus (agreement between people) and consistency (agreement across responses from one person). In general we find low consistency and consensus, potential confusion over mainstream features, and arguably poor design choices. In short, this preliminary evidence does not argue in favor of designing languages based on programmer preference. Sometime during the semester you will want to ask us questions about code. You might even think you've found a problem with a system being used in this course, and be inclined to submit a bug report. If you think you've found a problem, please do submit a report: you're doing us a favor, and we're grateful for it. However, before you do, please submit a good bug report. We'll write the rest of this document in terms of bug reporting, but the same principles apply even to asking a question. What follows below is general advice, not just for this course but for any system you might want to write a report about, ever. It's written in terms that make sense for programming tools, but you can generalize them to other systems without much trouble. A developer usually initially looks at the first two parts. More times than you might think, the seen output is actually correct. Then the developer knows that the problem is to educate you on why the system is behaving correctly, and your statement of why you thought it should do something else helps them understand your misunderstanding. If, however, the developer agrees that the seen output is not correct, now they have a problem to diagnose. Again, they will use all three parts. The first lets the developer try to get into the same state you're in. The second lets the developer confirm that they're in the same state. The third lets them understand what you think is wrong with this state. Yes, doing this last step is onerous. It's tempting to make it someone else's problem. But the more you make it someone else's problem, the longer it will take them to respond. Indeed, you should assume an exponential law: for every extra line of code, it may take twice as long to get a response. That's because your program, which is completely obvious to you, may take quite a while for your recipient to take into their heads and process. They don't know what's essential and what's incidental. So, before you submit the report, pare down. In fact, ideally, start from the empty buffer and add only what is absolutely necessary to manifest the error. As a bonus (to both the recipient and you), as you do this, sometimes you'll find that the actual problem was a bug in your code, not in the system that you're trying to report. For Pyret, you can report bugs directly, though you may want to check whether a similar issue has already been filed. Beware that any code you submit will be publicly visible. This course's primary learning goal is to teach you foundational programming skills in the context of algorithms and data structures. The course does this by interleaving the two and often using each one to motivate the other. Except where otherwise indicated, the sole assessment in the course will be homework assignments. These are take-home and open book relative to the policy on Honesty and Sharing. Occasionally you will be asked to do some readings, especially if they are found relevant based on questions and discussion in class. In keeping with the learning goals, the assignments focus on programming, algorithmics, or both. Usually you will implement a solution to some task, and then analyze its complexity. The purpose of these is to help you gain facility with thinking of software systems from the perspectives of both implementation and analysis. The work load in the course is uniformly distributed across the semester. Students can expect to spend about 10 hours each week on assignments. Combined with the 2.5 hours spent per week in class, this translates to approximately 180 hours over the course of the semester."

data TextData:
  | text(
      text-string :: String,
      sample-len :: Number,
      sample-step :: Number,
      char-dict :: StringDict<Number>,
      reverse-lookup :: List<Number>)
sharing:
  method get-example-begin-indices(self :: TextData) -> List<Number>:
    fun sampler(current-index :: Number, endpoint :: Number) -> List<Number>:
      if current-index < endpoint:
        link(current-index, sampler(current-index + self.sample-step, endpoint))
      else:
        empty
      end
    end
    sample = sampler(0, (string-length(self.text-string) - self.sample-len) - 1)
    L.shuffle(sample)
  end,

  method next-data-epoch(self :: TextData, num-examples :: Number) -> Object block:
    char-dict-size = self.char-dict.count()
    xs = TF.make-buffer([list: num-examples, self.sample-len, char-dict-size])
    ys = TF.make-buffer([list: num-examples, char-dict-size])

    example-begin-indices = self.get-example-begin-indices()

    self-indices = self.text-to-indices(self.text-string)

    for each(i from range(0, num-examples)) block:
      begin-index = example-begin-indices.get(num-modulo(i, example-begin-indices.length()))
      for each(j from range(0, self.sample-len)) block:
        xs.set(1, [list: i, j, self-indices.get(begin-index + j)])
      end
      ys.set(1, [list: i, self-indices.get(begin-index + self.sample-len)])
    end

    {xs: xs.to-tensor(), ys: ys.to-tensor()}
  end,

  method get-random-slice(self :: TextData) -> Object:
    max-index = (string-length(self.text-string) - self.sample-len) - 1
    start-index = num-random(max-index)
    end-index = start-index + self.sample-len

    string-slice = string-substring(self.text-string, start-index, end-index)
    index-slice = self.text-to-indices(string-slice)

    {
      text: string-slice,
      indices: index-slice
    }
  end,

  method text-to-indices(self :: TextData, string :: String) -> List<Number>:
    fun helper(characters :: List<String>) -> List<Number>:
      cases (List) characters:
        | empty => empty
        | link(f, r) =>
          link(
            self.char-dict.get-value(f),
            helper(r))
      end
    end

    helper(string-explode(string))
  end
end

fun list-of-chars-to-dict(list-of-chars :: List<String>) -> Object:
  for fold(
      base from {dict: [string-dict:], reverse: [list:]},
      char from list-of-chars):

    cases (Option) base.dict.get(char):
      | none =>
        {
          dict: base.dict.set(char, base.reverse.length()),
          reverse: base.reverse.append([list: char])
        }
      | some(_) => base
    end
  end
end

fun make-text-data(corpus :: String, sample-len :: Number, sample-step :: Number) -> TextData:
  result = list-of-chars-to-dict(string-explode(corpus))
  text(corpus, sample-len, sample-step, result.dict, result.reverse)
end

fun create-model(
    lstm-layer-sizes :: List<Number>,
    sample-length :: Number,
    char-set-size :: Number) -> Sequential block:
  doc: "Creates an LSTM model"

  model = TF.make-sequential({})

  size-count = lstm-layer-sizes.length()
  for each2(layer-size from lstm-layer-sizes, count from range(0, size-count)):
    lstm-layer = TF.lstm-layer({
        units: layer-size,
        returnSequences: (count < (size-count - 1)),
        inputShape: [raw-array: sample-length, char-set-size]
      })
    model.add(lstm-layer)
  end

  model.add(TF.dense-layer({units: char-set-size, activation: "softmax"}))

  model
end

fun compile-model(model :: Sequential, learning-rate :: Number):
  optimizer = TF.train-rmsprop(learning-rate, none, none, none, false)
  model.compile({optimizer: optimizer, loss: 'categoricalCrossentropy'})
end

fun fit-model(
    model,
    text-data :: TextData,
    num-epochs :: Number,
    examples-per-epoch :: Number,
    batch-size :: Number,
    validation-split :: Number):

  batches-per-epoch = examples-per-epoch / batch-size
  total-batches = num-epochs * batches-per-epoch

  for each(epoch from range(0, num-epochs)) block:
    print("Starting epoch #" + num-to-string(epoch) + "...")
    start-time = time-now()

    next-data = text-data.next-data-epoch(examples-per-epoch)
    xs        = next-data.xs
    ys        = next-data.ys

    model.fit(xs, ys,
      {
        epochs: 1,
        batchSize: batch-size,
        validationSplit: validation-split
      }, {(_,_): nothing})

    end-time = time-now()
    total-ms = end-time - start-time
    print(" Completed in " + num-to-string(total-ms) + "ms.")
  end
end

fun sample-on-probabilities(
    preds :: Tensor,
    temperature :: Tensor)
  -> Number:
  log-preds = TF.tensor-log(preds).divide(temperature)
  exp-preds = TF.tensor-exp(log-preds)
  sum-preds = TF.reduce-sum(exp-preds)
  new-preds = exp-preds.divide(sum-preds)

  TF.multinomial(new-preds, 1, none, true).data-sync().first
end

fun generate-text(
    model :: Sequential,
    text-data :: TextData,
    sentence-indices :: List<Number>,
    length :: Number,
    temperature :: NumPositive)
  -> String:

  temperature-scalar = TF.make-scalar(temperature)

  generation = for fold(
      base from {result: "", indices: sentence-indices},
      _ from range(0, length)) block:
    # Encode the current input sequence as a one-hot Tensor:
    input-buffer = TF.make-buffer([list: 1, text-data.sample-len, text-data.char-dict.count()])
    for each(i from range(0, text-data.sample-len)):
      input-buffer.set(1, [list: 0, i, base.indices.get(i)])
    end
    input = input-buffer.to-tensor()

    # Call model.predict() to get the probability values of the next character:
    output = model.predict(input, {})

    # Sample randomly based on the probability values:
    winner-index = num-exact(sample-on-probabilities(output.squeeze(none), temperature-scalar))
    winner-char = text-data.reverse-lookup.get(winner-index)

    {
      result: base.result + winner-char,
      indices: base.indices.drop(1).append([list: winner-index])
    }
  end

  generation.result
end

SAMPLE-LEN = 40
SAMPLE-STEP = 3

LEARNING-RATE = 0.01

NUM-EPOCHS = 5
EXAMPLES-PER-EPOCH = 512
BATCH-SIZE = 128
VALIDATION-SPLIT = 0.0625

GENERATED-LENGTH = 200
TEMPERATURE = 0.75

LSTM-LAYER-SIZES = [list: 128]

start-model = time-now()

text-data = make-text-data(DATASET, SAMPLE-LEN, SAMPLE-STEP)
model = create-model(LSTM-LAYER-SIZES, SAMPLE-LEN, text-data.char-dict.count())
compile-model(model, LEARNING-RATE)
fit-model(model, text-data, NUM-EPOCHS, EXAMPLES-PER-EPOCH, BATCH-SIZE, VALIDATION-SPLIT)

end-model = time-now()
total-model-time = end-model - start-model

print("Entire model generation took " + num-to-string(total-model-time) + "ms.")

seed-sentence = text-data.get-random-slice()
string-sentence = seed-sentence.text
sentence-indices = seed-sentence.indices

generate-text(model, text-data, sentence-indices, GENERATED-LENGTH, TEMPERATURE)
